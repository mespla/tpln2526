
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
        <link rel="prev" href="../speech/">
      
      
      
        
      
      
      <link rel="icon" href="../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.7.1">
    
    
      
        <title>Architectures for written-text processing - Natural Language Processing Techniques</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/main.484c7ddc.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
    <script>__md_scope=new URL("..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
  </head>
  
  
    <body dir="ltr">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#architectures-for-written-text-processing" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href=".." title="Natural Language Processing Techniques" class="md-header__button md-logo" aria-label="Natural Language Processing Techniques" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Natural Language Processing Techniques
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Architectures for written-text processing
            
          </span>
        </div>
      </div>
    </div>
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      
      
        <label class="md-header__button md-icon" for="__search">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        </label>
        <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
      
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href=".." title="Natural Language Processing Techniques" class="md-nav__button md-logo" aria-label="Natural Language Processing Techniques" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    Natural Language Processing Techniques
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href=".." class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Natural Language Processing Techniques, 2025-2026
  

    
  </span>
  
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../assignment-interpretability/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Assignment on Mechanistic Interpretability of Transformers
  

    
  </span>
  
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../assignment-searchinvectorialspace/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Assignment on text similarity using vectorial representations of text
  

    
  </span>
  
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../cl/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Introduction to computational linguistics and natural language processing (November 21, 2025)
  

    
  </span>
  
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../speech/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Architectures for Speech
  

    
  </span>
  
  

      </a>
    </li>
  

    
      
      
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  
  <span class="md-ellipsis">
    
  
    Architectures for written-text processing
  

    
  </span>
  
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  
  <span class="md-ellipsis">
    
  
    Architectures for written-text processing
  

    
  </span>
  
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#first-session-of-this-module-december-5-2025" class="md-nav__link">
    <span class="md-ellipsis">
      
        First session of this module (December 5, 2025)
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="First session of this module (December 5, 2025)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#before-text1" class="md-nav__link">
    <span class="md-ellipsis">
      
        Contents to prepare before the session on Dec 5
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#contents-for-the-in-person-session-on-dec-5" class="md-nav__link">
    <span class="md-ellipsis">
      
        Contents for the in-person session on Dec 5
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#second-session-december-12-2025" class="md-nav__link">
    <span class="md-ellipsis">
      
        Second session (December 12, 2025)
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Second session (December 12, 2025)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#before-text2" class="md-nav__link">
    <span class="md-ellipsis">
      
        Contents to prepare before the session on Dec 12
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#contents-for-the-in-person-session-on-dec-12" class="md-nav__link">
    <span class="md-ellipsis">
      
        Contents for the in-person session on Dec 12
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#third-session-december-19-2025" class="md-nav__link">
    <span class="md-ellipsis">
      
        Third session (December 19, 2025)
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Third session (December 19, 2025)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#before-text3" class="md-nav__link">
    <span class="md-ellipsis">
      
        Contents to prepare before the session on Jan 19
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#contents-for-the-in-person-session-on-dec-19" class="md-nav__link">
    <span class="md-ellipsis">
      
        Contents for the in-person session on Dec 19
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#fourth-session-january-14-2026" class="md-nav__link">
    <span class="md-ellipsis">
      
        Fourth session (January 14, 2026)
      
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#first-session-of-this-module-december-5-2025" class="md-nav__link">
    <span class="md-ellipsis">
      
        First session of this module (December 5, 2025)
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="First session of this module (December 5, 2025)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#before-text1" class="md-nav__link">
    <span class="md-ellipsis">
      
        Contents to prepare before the session on Dec 5
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#contents-for-the-in-person-session-on-dec-5" class="md-nav__link">
    <span class="md-ellipsis">
      
        Contents for the in-person session on Dec 5
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#second-session-december-12-2025" class="md-nav__link">
    <span class="md-ellipsis">
      
        Second session (December 12, 2025)
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Second session (December 12, 2025)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#before-text2" class="md-nav__link">
    <span class="md-ellipsis">
      
        Contents to prepare before the session on Dec 12
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#contents-for-the-in-person-session-on-dec-12" class="md-nav__link">
    <span class="md-ellipsis">
      
        Contents for the in-person session on Dec 12
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#third-session-december-19-2025" class="md-nav__link">
    <span class="md-ellipsis">
      
        Third session (December 19, 2025)
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Third session (December 19, 2025)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#before-text3" class="md-nav__link">
    <span class="md-ellipsis">
      
        Contents to prepare before the session on Jan 19
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#contents-for-the-in-person-session-on-dec-19" class="md-nav__link">
    <span class="md-ellipsis">
      
        Contents for the in-person session on Dec 19
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#fourth-session-january-14-2026" class="md-nav__link">
    <span class="md-ellipsis">
      
        Fourth session (January 14, 2026)
      
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              
              <article class="md-content__inner md-typeset">
                
                  



<h1 id="architectures-for-written-text-processing">Architectures for written-text processing<a class="headerlink" href="#architectures-for-written-text-processing" title="Permanent link">&para;</a></h1>
<div class="admonition danger">
<p class="admonition-title">Danger</p>
<p>These materials are temporary and incomplete. If you choose to read them, you do so at your own risk.</p>
</div>
<p>In this module, we will study some neural models used to process texts. The professor of this module is Juan Antonio P√©rez Ortiz. The module begins with a review of the functioning of logistic regression, which will help us establish the necessary knowledge to understand subsequent models. Next, we study in some detail <em>skip-grams</em>, one of the algorithms for obtaining non-contextual word <em>embeddings</em>. Then, we review the functioning of <em>feedforward</em> neural architectures and study their application to language models. The ultimate goal is to address the study of the most important architecture in current text processing systems: the transformer. Once we have studied these architectures, we will conclude with an analysis of the functioning of pretrained models (foundational models) in general, and language models in particular.</p>
<p>Class materials complement the reading of some chapters from a textbook ("Speech and Language Processing" by Dan Jurafsky and James H. Martin, third edition draft, available online) with annotations made by the professor.</p>
<h2 id="first-session-of-this-module-december-5-2025">First session of this module (December 5, 2025)<a class="headerlink" href="#first-session-of-this-module-december-5-2025" title="Permanent link">&para;</a></h2>
<h3 id="before-text1">Contents to prepare before the session on Dec 5<a class="headerlink" href="#before-text1" title="Permanent link">&para;</a></h3>
<p>The activities to complete before this class are:</p>
<ul>
<li>Reading and studying the contents of <a href="https://dlsi.ua.es/~japerez/materials/transformers/en/regresor/">this page</a> on logistic regression. As you will see, the page indicates which contents you should read from the book. After a first reading, read the professor's annotations, whose purpose is to help you understand the key concepts of the chapter. Then, perform a second reading of the book's chapter. In total, this part should take you about 4 hours üïíÔ∏è of work.</li>
<li>Watching and studying the video tutorials in this <a href="https://www.youtube.com/playlist?list=PL_lsbAsL_o2CTlGHgMxNrKhzP97BaG9ZN">official PyTorch playlist</a>. Study at least the first 4 videos (‚ÄúIntroduction to PyTorch‚Äù, ‚ÄúIntroduction to PyTorch Tensors‚Äù, ‚ÄúThe Fundamentals of Autograd‚Äù, and ‚ÄúBuilding Models with PyTorch‚Äù). In total, this part should take you about 2 hours üïíÔ∏è of work.</li>
<li>Reading and studying the contents of <a href="https://dlsi.ua.es/~japerez/materials/transformers/en/embeddings/">this page</a> on embeddings. As you will see, the page indicates which contents you should read from the book. After a first reading, read the professor's annotations to help you understand the key concepts of the chapter. Then, perform a second reading of the chapter from the book. In total, this part should take you about 3 hours üïíÔ∏è of work.</li>
<li>After completing the previous parts, take this <a href="https://forms.gle/V3U9MTHo7c9DNhkc6">assessment test</a> on these contents. There are few questions, and it will take you a few minutes.</li>
</ul>
<h3 id="contents-for-the-in-person-session-on-dec-5">Contents for the in-person session on Dec 5<a class="headerlink" href="#contents-for-the-in-person-session-on-dec-5" title="Permanent link">&para;</a></h3>
<p>In the in-person class (5 hours üïíÔ∏è long), we will see how to implement a logistic regressor in PyTorch by following the implementations of a binary logistic regressor <a href="https://colab.research.google.com/github/jaspock/me/blob/main/docs/materials/transformers/assets/notebooks/logistic.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"></a> and a multinomial one <a href="https://colab.research.google.com/github/jaspock/me/blob/main/docs/materials/transformers/assets/notebooks/softmax.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"></a> discussed in <a href="https://dlsi.ua.es/~japerez/materials/transformers/en/implementacion/#code-regressor">this section</a>. We will also explore an implementation of the skip-gram algorithm <a target="_blank" href="https://colab.research.google.com/github/jaspock/me/blob/main/docs/materials/transformers/assets/notebooks/skipgram.ipynb">
  <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a> discussed <a href="https://dlsi.ua.es/~japerez/materials/transformers/en/implementacion/#code-skipgrams">here</a>.</p>
<p>The idea is for you to study and slightly modify the notebooks we are working on. In a later class, a more advanced assignment involving modifying the transformer's code will be presented.</p>
<h2 id="second-session-december-12-2025">Second session (December 12, 2025)<a class="headerlink" href="#second-session-december-12-2025" title="Permanent link">&para;</a></h2>
<h3 id="before-text2">Contents to prepare before the session on Dec 12<a class="headerlink" href="#before-text2" title="Permanent link">&para;</a></h3>
<p>The activities to complete before this class are:</p>
<ul>
<li>Reading and studying the contents of <a href="https://dlsi.ua.es/~japerez/materials/transformers/en/ffw/">this page</a> on feedforward neural networks and their use as very basic language models. Perform at least two readings complemented with the professor's notes as in the previous point. In total, this part should take you about 2 hours üïíÔ∏è of work.</li>
<li>Reading and studying the contents of <a href="https://dlsi.ua.es/~japerez/materials/transformers/en/attention/">this page</a> as an introduction to transformers. As always, perform at least two readings complemented with the professor's notes. In total, this part should take you about 4 hours üïíÔ∏è of work.</li>
<li>After completing the previous parts, take this <a href="https://forms.gle/7KDwRtXcrpxsKjHp7">assessment test</a> on these contents. There are few questions, and it will take you a few minutes.</li>
<li>If you have time left, take the opportunity to review the contents of the first session.</li>
</ul>
<h3 id="contents-for-the-in-person-session-on-dec-12">Contents for the in-person session on Dec 12<a class="headerlink" href="#contents-for-the-in-person-session-on-dec-12" title="Permanent link">&para;</a></h3>
<p>In the in-person class (5 hours üïíÔ∏è in duration), we will see how to implement in PyTorch a language model based on a feedforward neural network <a target="_blank" href="https://colab.research.google.com/github/jaspock/me/blob/main/docs/materials/transformers/assets/notebooks/ffnn.ipynb">
  <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a>, and a transformer <a target="_blank" href="https://colab.research.google.com/github/jaspock/me/blob/main/docs/materials/transformers/assets/notebooks/transformer.ipynb">
  <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a> following the implementations discussed in <a href="https://www.dlsi.ua.es/~japerez/materials/transformers/implementacion/#code-transformer">this section</a> and the next two.</p>
<p>A very similar notebook to the one of the feedforward neural network but focused on the self-supervised learning of word embeddings <a target="_blank" href="https://colab.research.google.com/github/jaspock/me/blob/main/docs/materials/transformers/assets/notebooks/embeddings-ffnn.ipynb">
  <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a> is also available.</p>
<p>The idea is for you to study and slightly modify the notebooks we are working on. We will also present the <a href="../assignment-interpretability/">assignment</a> on mechanistic interpretability you need to submit for this module of the course.</p>
<h2 id="third-session-december-19-2025">Third session (December 19, 2025)<a class="headerlink" href="#third-session-december-19-2025" title="Permanent link">&para;</a></h2>
<h3 id="before-text3">Contents to prepare before the session on Jan 19<a class="headerlink" href="#before-text3" title="Permanent link">&para;</a></h3>
<p>The activities to complete before this class are:</p>
<ul>
<li>Reading and studying the contents of <a href="https://dlsi.ua.es/~japerez/materials/transformers/en/attention2/">this page</a> on the complete transformer model (with encoder and decoder) and the possible uses of an architecture that only includes the encoder. As you will see, the page indicates which contents you should read from the book. In particular, you will need to read some sections of the chapter on machine translation and others from the chapter on pretrained models, in addition to standalone sections on <em>beam search</em> and subword tokenization. After a first reading, read the professor's annotations to help you understand the key concepts of each section. Then, perform a second reading of the book's contents. In total, this part should take you about 4 hours üïíÔ∏è of work.</li>
<li>Watching and studying Jesse Mu's lecture titled ‚Äú<a href="https://youtu.be/SXpJ9EmG3s4?si=j4B1U2Z-JCyYJwlc">Prompting, Reinforcement Learning from Human Feedback</a>‚Äù from Stanford's CS224N course in 2023 about language models based on the transformer's decoder. This should take you about 2 hours üïíÔ∏è of work, as you'll need to take notes so you don't have to rewatch the video when reviewing. Downloading the <a href="https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1234/slides/cs224n-2023-lecture11-prompting-rlhf.pdf">slides</a> and annotating them may be helpful. Regarding the topic discussed between minutes 39 and 46, you can simply focus on the basic ideas, as the reinforcement learning equations are not a priority topic for this course and will be covered in other courses. It's important to review what you've already studied about <a href="https://dlsi.ua.es/~japerez/materials/transformers/en/attention/">transformers</a> as a language model based on the decoder before watching the video. Don't be confused by encoder-based models also sometimes being called language models. This video discusses the properties of decoder-based models initially trained to predict the next token in a sequence.</li>
<li>Study the description of <a href="https://dlsi.ua.es/~japerez/materials/transformers/en/attention2/#multilingual-models">multilingual models</a> in this section of one of the pages on transformers. It's a brief section that will take you about üïíÔ∏è 15 minutes.</li>
<li>After completing the previous parts, take this <a href="https://forms.gle/GRK5SLc3STkup8at9">assessment test</a> on these contents. There are few questions, and it will take you a few minutes.</li>
<li>If you have time left, take the opportunity to review all the contents from previous sessions.</li>
</ul>
<h3 id="contents-for-the-in-person-session-on-dec-19">Contents for the in-person session on Dec 19<a class="headerlink" href="#contents-for-the-in-person-session-on-dec-19" title="Permanent link">&para;</a></h3>
<p>In the in-person class (5 hours üïíÔ∏è in duration), we will see how to implement on top of our transformer architecture code both a language model based on a decoder <a target="_blank" href="https://colab.research.google.com/github/jaspock/me/blob/main/docs/materials/transformers/assets/notebooks/lmgpt.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a> and a named entity recognition model <a target="_blank" href="https://colab.research.google.com/github/jaspock/me/blob/main/docs/materials/transformers/assets/notebooks/nerbert.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a> based on an encoder.</p>
<p>We will take the opportunity to review some aspects of the code from previous sessions and relate theoretical aspects with practical ones.</p>
<h2 id="fourth-session-january-14-2026">Fourth session (January 14, 2026)<a class="headerlink" href="#fourth-session-january-14-2026" title="Permanent link">&para;</a></h2>
<p>This fourth session is actually the first and only session on the topic of speech. See the page on <a href="../speech/">speech</a> to view the contents prior to this session.</p>












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    
      
      
      <script id="__config" type="application/json">{"annotate": null, "base": "..", "features": [], "search": "../assets/javascripts/workers/search.2c215733.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script>
    
    
      <script src="../assets/javascripts/bundle.79ae519e.min.js"></script>
      
    
  </body>
</html>