{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Natural Language Processing Techniques, 2025-2026","text":""},{"location":"#homework-assignments","title":"Homework Assignments","text":"<ul> <li>Assignments before the class of Jan 14, 2026: Read the materials related to speech recognition, linked in this section and then complete this test (deadline: 23:59 CET, Jan 13 2026).</li> <li>Assignments before the class of Dec 19, 2025: Read the materials related to full encoder-decoder transformer models, encoder-like transformers, large language models and multilingual models linked in this section and then complete this test (deadline: 23:59 CET, Dec 18 2025).</li> <li>Assignments before the class of Dec 12, 2025: Read the materials related to feedforward neural networks and transformers, linked in this section and then complete this test (deadline: 23:59 CET, Dec 11 2025).</li> <li>Assignments before the class of Dec 5, 2025: read the materials related to logistic regressors, embeddings and PyTorch linked in this section and then complete this test (deadline: 23:59 CET, Dec 04 2025). </li> <li>Assignments before the class of Nov 21, 2025: read the contents of section Introduction to computational linguistics and natural language processing; after that, complete this test (deadline: 23:59 CET, Nov 20 2025).</li> </ul>"},{"location":"#course-syllabus-and-rules","title":"Course Syllabus and Rules","text":"<p>These are the teaching materials for the course T\u00e9cnicas de Procesamiento del Lenguaje Natural, coordinated by professor Juan Antonio P\u00e9rez Ortiz (@japer3z) at the University of Alacant and also taught by professor Miquel Espl\u00e0 Gomis.</p> <p>For information regarding the course assessment, please refer to the course official info page. Some additional aspects not covered in the syllabus include:</p> <ul> <li>Practical assignments are to be completed individually or in pairs, as indicated in each assignment description. Each of the course blocks (computational linguistics, text, speech) will have one or more practical assignments. The assignments in the first block contribute 25% to the final grade, the second block assignments contribute 55%, and the third block assignments contribute 20%. In case, the speech block has no practical assignments, the contributions of the first and second blocks will be 22% and 78%, respectively.</li> <li>Attendance in practical sessions is mandatory. Roll call will be taken in each in-person session. A maximum of 1 unexcused absence is allowed. If a student accumulates more unexcused absences than the allowed limit, they will not be able to pass the practical component of the course in the C2 examination. However, they will have the opportunity to pass it in the C4.</li> </ul> <p>The source code of these pages, written in markdown for MkDocs, is available on GitHub.</p> <p>You can obtain a local copy of these pages (e.g., for offline access) by executing:</p> <pre><code>wget --mirror --no-parent --convert-links --page-requisites https://mespla.github.io/tpln2526\n</code></pre> <p>Please note that the content may change throughout the course.</p> <p>The course has three blocks:</p> <ul> <li>Block 1: Introduction to computational linguistics and natural language processing</li> <li>Block 2: Architectures for written-text processing</li> <li>Block 3: Architectures for speech</li> </ul>"},{"location":"assignment-interpretability/","title":"Assignment on Mechanistic Interpretability of Transformers","text":"<p>Mechanistic interpretability in the context of artificial intelligence seeks to provide a motivated explanation of how machine learning models function. It is a crucial approach to building trust in systems and inducing certain behaviors in them. Within the field of mechanistic interpretability, there are many techniques that can be applied to transformers. Here, we will focus on activation patching.</p> <p>Activation patching intervenes in a specific model activation by replacing a corrupted activation with a clean activation. The effect of this change on the model output is then measured. This helps us identify which activations are important for the model's output and locate possible causes of prediction errors.</p> <p>In this practice, you will write code to run the smallest version of GPT2 (use the string <code>gpt2</code> in the code) with two different inputs: two texts that differ by only one token. The idea is that when the corrupted input is fed to the model, we will intervene in the embedding after a certain layer (one at a time) and patch it with the corresponding embedding from the clean run. Then we will measure how much the prediction of the next token changes compared to the clean run. If the change is significant, we can be confident that the altered activation is important for the prediction. This patching process will be performed for each layer of the model and for each token in the input. With all this information, we will generate a heatmap and draw conclusions. For reasons you will soon understand, both texts must have the same number of tokens.</p>"},{"location":"assignment-interpretability/#analysis-example","title":"Analysis Example","text":"<p>Here is an example to better understand the task. Consider the following input text: \"Michelle Jones was a top-notch student. Michelle\". If we feed it to GPT2 and study the model's emitted probability for the token following the second appearance of Michelle, we obtain the following (only the 20 most probable tokens are shown): </p> Position Token index Token Probability 1 373 was 0.1634 2 5437 Jones 0.1396 3 338 's 0.0806 4 550 had 0.0491 5 318 is 0.0229 6 290 and 0.0227 7 11 , 0.0222 8 531 said 0.0134 9 468 has 0.0120 10 635 also 0.0117 11 1625 came 0.0091 12 1297 told 0.0084 13 1422 didn 0.0070 14 2993 knew 0.0067 15 1816 went 0.0061 16 561 would 0.0061 17 3111 worked 0.0055 18 750 did 0.0054 19 2486 Obama 0.0053 20 2492 wasn 0.0050 <p>As expected, the token \"Jones\" has a notably high probability. Now consider the corrupted input \"Michelle Smith was a top-notch student. Michelle\". If we provide this input to GPT2, we expect the probability of \"Jones\" as the next token to be much lower than before, while the probability of \"Smith\" will be much higher, which (you can verify) indeed happens. However, we want to go further and understand which embeddings most influence this difference. Given that both inputs have 11 tokens (we will explain how to verify this later) and the transformer in the small GPT2 model has 12 layers, if we focus on the embeddings obtained at the output of each layer, we can patch 11\u00d712 = 132 different embeddings. Therefore, we will calculate 132 times the difference between the logit of \"Smith\" and the logit of \"Jones\" in the output of the last token of the input (\"Michelle\") in the corrupted model. Note that we could also calculate the differences after applying the softmax function, but we will not do so here.</p> <p>A heatmap representation of the result is shown below:</p> <p></p> <p>In such a graph, due to the attention mask and the arrangement of the layers, information flows from left to right and top to bottom. You can see that intervening in the first column has no effect on the prediction of the next token, which makes sense, as the embeddings patched have exactly the same values in the clean and corrupted models, given the same preceding context. There also seem to be no changes when patching embeddings from the third to the penultimate column. However, note how intervening in the embeddings of many layers of the second token shifts the prediction towards \"Jones\" (the color darkens as the logit difference between \"Smith\" and \"Jones\" becomes negative because \"Jones\" has a higher logit). Modifying the embeddings of the last layers of the second token has much smaller effects, as the embedding barely influences the sequence's future. At the last position (\"Michelle\"), the embeddings of the final layers seem to anticipate the token to predict.</p> <p>Some additional corrupted texts that may be interesting to explore are, for example, \"Jessica Jones was a top-notch student. Michelle\" or \"Michelle Smith was a top-notch student. Jessica\".</p>"},{"location":"assignment-interpretability/#submission","title":"Submission","text":"<p>In this assignment, your task is to write the code to generate graphs and probabilities like those above, propose your own clean and corrupted texts (try to be creative and avoid studying very similar texts or phenomena), perform a similar analysis, and write a report in a document of 1000\u20131500 words (both limits are strict). In this document, you should present and explain the implemented code, along with your approach, results, and relevant conclusions. Original ideas and additional experiments are welcome. The document in PDF format must be submitted via the UACloud evaluation tools before 23:55 on Sunday, January 26, 2026 (Alicante time). The assignment must be done in pairs. Remember to include both authors' names in the document.</p>"},{"location":"assignment-interpretability/#base-code","title":"Base Code","text":"<p>The base code we will use is from the GPT2 implementation found in Andrej Karpathy's minGPT repository. His code inspired our transformer model code, so it should not be difficult to understand. You can clone the repository on your computer or work in a Google Colab notebook as described below.</p> <p>Due to changes in external elements, the current code does not work as is. To make it work, you need to change line 200 of the <code>mingpt/model.py</code> file from:</p> <pre><code>assert len(keys) == len(sd)\n</code></pre> <p>to:</p> <pre><code>assert len(keys) == len([k for k in sd if not k.endswith(\".attn.bias\")])\n</code></pre>"},{"location":"assignment-interpretability/#tokenization","title":"Tokenization","text":"<p>The GPT2 model uses a BPE-based tokenizer that segments the input text into words or smaller units depending on their frequency. The minGPT code allows downloading this tokenizer and using it to segment texts. The following code shows how to tokenize a text to obtain its indices and vice versa.</p> <pre><code>from mingpt.bpe import BPETokenizer\n\ninput = \"Michelle Jones was a top-notch student. Michelle\"\nprint(\"Input:\", input)\nbpe = BPETokenizer()\n# bpe() gets a string and returns a 2D batch tensor \n# of indices with shape (1, input_length)\ntokens = bpe(input)[0]\nprint(\"Tokenized input:\", tokens)\ninput_length = tokens.shape[-1]\nprint(\"Number of input tokens:\", input_length)\n# bpe.decode gets a 1D tensor (list of indices) and returns a string\nprint(\"Detokenized input from indices:\", bpe.decode(tokens))  \ntokens_str = [bpe.decode(torch.tensor([token])) for token in tokens]\nprint(\"Detokenized input as strings: \" + '/'.join(tokens_str))\n</code></pre>"},{"location":"assignment-interpretability/#implementation-details","title":"Implementation Details","text":"<p>The following are some implementation details that may be useful but are not required to follow.</p> <p>To write code that allows activation patching, you will need to focus on the files <code>mingpt/model.py</code> and <code>generate.ipynb</code>. If you are working locally without using a notebook (recommended), copy the code from <code>generate.ipynb</code> into a <code>generate.py</code> file that you can execute from the command line.</p> <p>You can also work directly in a Google Colab session. Here is a project (access with your <code>gcloud.ua.es</code> account) with instructions on how to use it for development. However, developing locally is much more convenient (among other things, you can work with a better text editor than Colab\u2019s and also debug). Even if you don\u2019t have a GPU, the code runs fine on a CPU and only takes a few seconds longer than on a GPU, as it only works with one text and a not excessively large model.</p> <p>Add to the transformer\u2019s <code>forward</code> function code that allows saving (depending on the value of a boolean flag passed as a parameter) the activations of each layer and each position into an instance variable. Remember to make a deep copy of the embeddings rather than only saving a reference that could be overwritten later; for this, check PyTorch\u2019s <code>.detach().clone()</code> sequence of calls. Also, add code that allows (again based on a boolean parameter) patching the embedding of a specific layer and position.</p> <p>Additionally, modify the <code>forward</code> function to store the logits of the last token, which contain the information we are interested in regarding the prediction of the next token. You can save this information in an attribute that can later be accessed from outside the class. Note that you only need the vector corresponding to the last token.</p> <p>Add code to the <code>generate.py</code> file to tokenize the clean text, pass it through the model via the <code>generate</code> function (asking the model to save the intermediate embeddings), and display the most probable continuations based on the logits of the last token. Keep in mind that if you want to know the probability of a continuation like the token \"Jones\", for example, you need to find the index of that token in the vocabulary by prefixing it with a space (<code>index = bpe(' Jones')</code>). This is because the BPE tokenizer handles tokens at the beginning of a sequence differently from those in the middle. Once you have the token\u2019s index, you can access the corresponding position in the logits vector and obtain the unnormalized probability of it being the continuation.</p> <p>Then, you can work with the corrupted text. Include a nested loop that iterates over all layers and all positions and calls <code>generate</code> each time, passing the layer and position where the intervention should be performed. At each step, compute the appropriate logit difference and store it in a difference matrix.</p> <p>Finally, use the <code>matshow</code> function from <code>matplotlib</code> to visualize the difference matrix.</p>"},{"location":"assignment-interpretability/#a-more-informal-explanation","title":"A More Informal Explanation","text":"<p>The following informal explanation may help you better understand the objective of the assignment.</p> <p>For simplicity, consider the sequence \"a b c\" and its corrupted version \"d e f.\" In general, there will be many more tokens in common, but this makes the following discussion clearer. Assume the transformer-based neural model has 5 attention layers. We want to study which embeddings are important for predicting that the token \"X\" follows these sequences.</p> <p>First, modify the transformer\u2019s <code>forward</code> function (in the <code>GPT</code> class) to store (e.g., in a list of lists of tensors) the 3\u00d75=15 embeddings generated at the output of each layer when processing the sequence \"a b c\". The assignment provides some details because you cannot simply store a reference to the tensors, as they will be modified the next time <code>forward</code> is called. Instead, you need to clone the tensors (a \"defensive copy\"). This will leave you with the 15 tensors (embeddings) for the clean sequence.</p> <p>Also, save the logits after the last layer. In particular, you only need the logits for the final position (i.e., those corresponding to the token \"c\"), which provide a measure of the probability of the next token, i.e., the token following \"c\". Remember that these logits are not actual probabilities (they are values like -11.1, -0.5, 0.78, or 2.32323) because the softmax function has not been applied, but working with them is more convenient due to their broader range. However, the study could equally be conducted using strict probabilities. In reality, you don\u2019t even need to save all the logits, only the scalar corresponding to token \"X,\" as it is the only one you will use later.</p> <p>Now feed the model the corrupted version \"d e f\", ensuring that it does not overwrite the stored embeddings from the clean sequence. The corrupted sequence must have the same number of tokens as the clean one for the following discussion to make sense. The idea is to modify only one of the 15 embeddings generated while processing the corrupted sequence. For example, if we focus on the embedding of the first token (\"d\") after the first layer, the <code>forward</code> function should operate \"almost\" normally. However, when obtaining the output of the first layer and before passing it as input to the second layer, the embedding corresponding to the first word (and only that) should be modified and replaced with the corresponding embedding (from the same layer and position) saved for the clean sequence (in this case, the embedding saved after the first layer for the token \"a\"). This ensures that the second layer receives as input the embedding generated for \"a\" instead of \"d\".</p> <p>After intervening in the embedding of position 1 after layer 1, the rest of the model operates without any \"hiccups.\" As before, examine the logits for predicting the token following the last token of the corrupted sequence (i.e., \"f\"). Focus on the value of the logit for the prediction of token \"X\". The difference between this value and the one saved for the clean sequence provides insight into the relevance of the embedding at layer 1, position 1, for predicting token \"X\". The assignment shows that some embeddings are much more relevant than others, and you need to conduct a similar study with different sequences.</p> <p>If you repeat the above operation for the other 14 embeddings (calling the <code>forward</code> function 14 more times), you will end up with 15 logit differences (15 scalar values) that can be represented in a 3\u00d75 heatmap as seen above.</p> <p>Finally, note that this discussion simplifies the task described earlier in the assignment. There, it was proposed to calculate the difference between the logit of \"Smith\" and the logit of \"Jones\" in the output of the last token in the corrupted model. This approach provides slightly more information than the difference explained here, which is the difference between the prediction of a single token (\"Jones\") in the clean and corrupted sequences, rather than two tokens in the corrupted sequence. Either approach is valid for arriving at the conclusions we are interested in: that in the corrupted sequence, the logit of \"Jones\" becomes much lower except for certain interventions. If you want your heatmap to match the one in the assignment, follow the approach based on the two tokens \"Jones\" and \"Smith\".</p>"},{"location":"assignment-interpretability/#further-knowledge","title":"Further Knowledge","text":"<p>The above is just one of many analyses proposed within mechanistic interpretability. For this assignment, you are not expected to go beyond this. However, if you are interested in learning about a couple more analyses, you can check out this tutorial. Note that although the tutorial uses a library for activation patching, in this assignment, you are not allowed to use any library for this and must implement it directly in the minGPT code. A much more detailed review of mechanistic interpretability can be found in this work by Neel Nanda.</p>"},{"location":"assignment-searchinvectorialspace/","title":"Assignment on text similarity using vectorial representations of text","text":"<p>In Natural Language Processing (NLP), one of the primary challenges is how to represent human language in a form that computers can process. This is where vectorized representations of text come into play. Text data, such as sentences, paragraphs, or entire documents, need to be converted into numerical formats to be understood by machine learning algorithms. Vectorization transforms raw text into fixed-length, dense, or sparse vectors that capture the semantic properties of the words or sentences, enabling machines to perform tasks like classification, similarity search, and recommendation.</p> <p>The methods you'll explore in this assignment \u2014 Bag of Words (BoW), TF-IDF, and Sentence Embeddings (SBERT) \u2014 are foundational techniques in NLP, each with its own strengths and weaknesses. Understanding these representations is critical for a wide range of applications in the field of NLP.</p>"},{"location":"assignment-searchinvectorialspace/#introduction-to-the-work","title":"Introduction to the work","text":"<p>In this task, you will work with a dataset containing a collection of article titles and their corresponding abstracts. This information has been extracted from the proceedings of the EMNLP international conference for the years 2016, 2017, and 2018. Your goal is to create vectorized representations of these articles, which can then be used to identify similarities between titles or abstracts of different papers.</p> <p>As part of this assignment, you will explore various strategies for constructing these vector representations and use them to compare the similarity between text fragments. Specifically, you will:</p> <ol> <li>Evaluate two word-based methods for generating sparse vector representations of text: bag-of-words and TF-IDF-weighted word vectors.</li> <li>Use pre-trained embedding models to generate dense representations of the same text fragments.</li> </ol> <p>This assignment will be completed in pairs. To make your work more manageable, a CoLab notebook has been provided. It includes the basic structure of the tasks, along with code snippets to help you complete the assignment. You will use this CoLab notebook to:</p> <ul> <li>Complete the required tasks.</li> <li>Describe the results you obtain.</li> </ul> <p>Remember that you will need to be logged with your GCloud account to access the CoLab notebook.</p>"},{"location":"assignment-searchinvectorialspace/#submission","title":"Submission","text":"<p>In this assignment, your task is to implement the different strategies to produce vector representations of text and evaluate them with the dataset described in this document. You will also be required to try the different configurations and observe the results obtained. A base CoLab notebook is provided that contains the basic structure of the work to be done in this assignment. Follow the instructions in this document to both implement the solutions and to describe the results obtained and the conclusions drawn from this work. Once this is done, you should download the resulting CoLab notebook and submit it via the UACloud Evaluation tools before 23:59 on Sunday, December 14, 2025. The assignment must be done in pairs. Remember to include both authors' names in the notebook.</p>"},{"location":"assignment-searchinvectorialspace/#part-1-bag-of-words-bow-and-tf-idf-vectorization-with-scikit-learn","title":"Part 1: Bag of Words (BoW) and TF-IDF Vectorization with Scikit-Learn","text":"<p>In this section of the assignment, you will explore two widely used methods for vectorizing text: Bags of Words (BoW) and TF-IDF (Term Frequency-Inverse Document Frequency). Both techniques transform text into numerical vectors, but they capture different characteristics of the data.</p> <ul> <li>BoW: Text fragments are converted into sparse vectors that list the words appearing in the text along with their respective counts. This method focuses exclusively on word frequency within each text fragment.</li> <li>TF-IDF: This method builds on BoW by also considering the importance of a word across the entire corpus. It adjusts word counts based on how frequently the word appears in other documents, giving more weight to distinctive terms.</li> </ul> <p>You will use both approaches to vectorize the titles and abstracts of research papers and then perform a similarity search. Your task will be to identify the top 3 most similar documents for a given query.</p>"},{"location":"assignment-searchinvectorialspace/#step-1-preprocessing-the-data","title":"Step 1: Preprocessing the Data","text":"<p>Before applying either BoW or TF-IDF, it is important to preprocess the text data to ensure that it is in a usable form. The dataset you will be working with is in the JSON format. In the CoLab mentioned above you have the code snippet to download and store it locally. For each research paper in the dataset, the JSON file contains:</p> <ol> <li>The title of the paper.</li> <li>The abstract of the paper.</li> <li>The URL of the paper.</li> <li>The venue in which it was presented.</li> <li>The year of publication of the work.</li> </ol> <p>From these fields, you will only build vectorized representations on the title and the abstract. To do so, you can concatenate them in a single string:</p> <pre><code>   df['text'] = df['title'] + ' ' + df['abstract']\n</code></pre>"},{"location":"assignment-searchinvectorialspace/#step-2-building-the-bow-and-tf-idf-representations","title":"Step 2: Building the BoW and TF-IDF Representations","text":"<p>You will now create vectorized representations of the documents using Bag of Words (BoW) and TF-IDF. Both techniques are implemented in the <code>scikit-learn</code> library. To build BoW vectors, you will be using the CountVectorizer class, that automatically transforms a collection of fragment of text into a matrix of vectors consisting of a set of vectors containing the occurrences of each word. For the TF-IDF vectors, you will be using the TfidfVectorizer class. You can use the method <code>fit_transform</code>, that takes a list of strings and creates the set of vectors representing a collection of documents. Have a look to the vectors obtained and comment in the notebook their format and their size. Have a look to the documentation of these vectorizers to better understand what do they mean.</p> <p>A relevant aspect when producing vectorized representations building on words is text preprocessing. These vectorization methods allow some level of customization of the pre-processing steps carried out. For example, text is lowercased by default. In the same way, punctuation is ignored. However, some of these parameters can be modified; try, at least, enabling and disabling:</p> <ul> <li>Lowercasing the text</li> <li>Removing stop-words (you can use <code>stop_words='english'</code> in <code>CountVectorizer</code> or <code>TfidfVectorizer</code> to automatically handle this step)</li> </ul> <p>Analyse the differences obtained when changing these parameters and provide a short discussion of the results.</p>"},{"location":"assignment-searchinvectorialspace/#step-3-similarity-search","title":"Step 3: Similarity Search","text":"<p>After creating the vector representations for the entire dataset, you will perform vector comparisons to identify similarities between these representations. In this step, you will compare the representations of three new queries against the entire dataset. Each query consists of the title and abstract of a paper published in different journals and international conferences. These queries are:</p> <ul> <li>Query 1:<ul> <li>Title: QUALES: Machine translation quality estimation via supervised and unsupervised machine learning.</li> <li>Abstract: The automatic quality estimation (QE) of machine translation consists in measuring the quality of translations without access to human references, usually via machine learning approaches. A good QE system can help in three aspects of translation processes involving machine translation and post-editing: increasing productivity (by ruling out poor quality machine translation), estimating costs (by helping to forecast the cost of post-editing) and selecting a provider (if several machine translation systems are available). Interest in this research area has grown significantly in recent years, leading to regular shared tasks in the main machine translation conferences and intense scientific activity. In this article we review the state of the art in this research area and present project QUALES, which is under development. </li> </ul> </li> <li>Query 2:<ul> <li>Title: Learning to Ask Unanswerable Questions for Machine Reading Comprehension</li> <li>Abstract: Machine reading comprehension with unanswerable questions is a challenging task. In this work, we propose a data augmentation technique by automatically generating relevant unanswerable questions according to an answerable question paired with its corresponding paragraph that contains the answer. We introduce a pair-to-sequence model for unanswerable question generation, which effectively captures the interactions between the question and the paragraph. We also present a way to construct training data for our question generation models by leveraging the existing reading comprehension dataset. Experimental results show that the pair-to-sequence model performs consistently better compared with the sequence-to-sequence baseline. We further use the automatically generated unanswerable questions as a means of data augmentation on the SQuAD 2.0 dataset, yielding 1.9 absolute F1 improvement with BERT-base model and 1.7 absolute F1 improvement with BERT-large model.</li> </ul> </li> <li>Query 3:<ul> <li>Title: Unsupervised Neural Text Simplification</li> <li>Abstract: The paper presents a first attempt towards unsupervised neural text simplification that relies only on unlabelled text corpora. The core framework is composed of a shared encoder and a pair of attentional-decoders, crucially assisted by discrimination-based losses and denoising. The framework is trained using unlabelled text collected from en-Wikipedia dump. Our analysis (both quantitative and qualitative involving human evaluators) on public test data shows that the proposed model can perform text-simplification at both lexical and syntactic levels, competitive to existing supervised methods. It also outperforms viable unsupervised baselines. Adding a few labelled pairs helps improve the performance further.</li> </ul> </li> </ul> <p>To do so, you will have to use the method <code>transform</code> of the vectorization object used to produce the vectors of the original dataset. The difference between the <code>fit_transform</code> and <code>transform</code> methods is that the first learns the vocabulary from the dataset and then produces the vectorized representations of the text fragments, while the second one uses the vocabulary already learned to produce the representation for new text fragments.</p> <p>Finally, to compare the vector representations of the queries against the entire dataset, you can use the cosine similarity metric. This metric computes a similarity score ranging from 1 to -1 for each pair of vectors, although negative values are rare when comparing text fragments. The interpretation of the cosine similarity score is as follows:</p> <ul> <li>A value close to 1 indicates that the two vector representations are highly similar.</li> <li>A value close to 0 indicates that they are barely related.</li> </ul> <p>You can use the class <code>sklearn.metrics.pairwise.cosine_similarity</code> to compute the pairwise cosine similarities across two lists of vectors. A code snippet is provided in the CoLab notebook mentioned above to sort the results obtained and to print the top 3 highest-scored matches.</p>"},{"location":"assignment-searchinvectorialspace/#step-4-analysis-of-the-results-obtained","title":"Step 4: Analysis of the results obtained","text":"<p>Spend some time analysing the results obtained with the two techniques and with the different parameters you modified. Describe your impression about the relation between the papers in the query and those found with the different approaches. You can also comment on how informative or easy to interpret are the scores Add some discussion on your conclusions in the notebook.</p>"},{"location":"assignment-searchinvectorialspace/#part-2-sentence-embeddings-with-sentencetransformers","title":"Part 2: Sentence Embeddings with SentenceTransformers","text":"<p>In this section, you will try using  sentence embeddings for representing text with dense vectors. You will be using embedding models as provided through the SentenceTransformers library. Have a look to the general description to use this library in the link provided. In addition to the general description provided through the link, you can find a description of some interesting use cases, such as:</p> <ul> <li>Semantic Textual Similarity,</li> <li>Semantic Search,</li> <li>Retrieve &amp; Re-Rank,</li> <li>Clustering,</li> <li>Paraphrase Mining,</li> <li>Translated Sentence Mining,</li> <li>Image Search.</li> </ul> <p>Have a look to them for a better understanding of the capabilities of this type of embedding models.</p> <p>Note that, unlike BoW and TF-IDF, sentence embeddings capture the semantic meaning of sentences and documents, making them more powerful for tasks like similarity search. You will use the SentenceTransformers library to generate embeddings for the dataset used in Part 1 and will again run queries to find similar documents.</p>"},{"location":"assignment-searchinvectorialspace/#step-1-trying-a-general-purpose-small-monolingual-model","title":"Step 1: Trying a general purpose small monolingual model","text":"<p>We will first try a small model that allows to build semantic embeddings for English: the model <code>all-MiniLM-L6-v2</code>. You have a nice example on how to obtain the n-top matches for a query search in a collection of embeddings using the cosine similarity at: https://www.sbert.net/examples/applications/semantic-search/README.html.</p> <p>Note that in this case you will not need to specify any data preprocessing details. The pre-trained model already includes a sub-word tokenizer and takes care of this. Given that these components are trained, changing, for example, capitals, or removing words, could affect negatively to the performance of the model and the quality of the resulting embeddings.</p> <p>Run the semantic search, and compare the results obtained to those obtained previously, and discuss the differences observed. Also, have a look to the size and the format of the embeddings. Can you draw any conclusions from this inspection?</p>"},{"location":"assignment-searchinvectorialspace/#step-2-comparing-other-models","title":"Step 2: Comparing other models","text":"<p>The <code>SentenceTransformers</code> library provides easy access to a number of pretrained text embedding models. Try reproducing the experiment carried out in the previous step using other models. As already mentioned, the <code>all-MiniLM-L6-v2</code> model is rather small and general purpose. By small it should also be understood that the size of the embeddings is small, but also that the amount of text that can parse is rather small (256 sub-word tokens, according to the documentation in the library website). You can try with larger models, such as <code>all-distilroberta-v1</code>, and also with specific purpose: the <code>SPECTER</code> model is specifically aimed at detecting the similarity between two scientific papers.</p> <p>Again, try to draw conclusions from the use of these different models: what is your use experience, what is the impact in the results obtained, etc. Try to describe these conclusions in the notebook.</p>"},{"location":"assignment-searchinvectorialspace/#step-3-moving-to-a-multilingual-environment","title":"Step 3: Moving to a multilingual environment","text":"<p>Finally, you will explore the use of cross-lingual embedding models. The <code>SentenceTransformers</code> library provides a few pre-trained models of this type that enable the comparison of embeddings obtained from text fragments in different languages. You can try, for example, the model <code>distiluse-base-multilingual-cased-v1</code>. In order to be able to test the performance of these cross-lingual model, a new dataset has been created with the exact same format as the first one, but with papers extracted from the SEPLN journal. This journal allows papers in Spanish and in English. Try following the same steps you followed before with and search for the same query papers, but now on this bilingual dataset. Compare the results obtained when using a monolingual model and a cross-lingual model to obtain the embeddings. Once more, comment on your observations and the conclusions you are able to draw from them.</p>"},{"location":"assignment-searchinvectorialspace/#concluding-remarks","title":"Concluding remarks","text":"<p>After this evaluation, you have explored different strategies to obtain vector representations of text. End your notebook with a general overview of the work done and your experience in comparing these models. Try to identify the strengths and the weaknesses of the different methods compared and discuss them.</p>"},{"location":"cl/","title":"Introduction to computational linguistics and natural language processing (November 21, 2025)","text":"<p>Computational linguistics (CL) is a branch of linguistics that focuses on the theoretical understanding of language through computational models. In contrast, natural language processing (NLP) is an interdisciplinary field within artificial intelligence (AI) that aims to use computational models to process and generate language efficiently. NLP intersects with machine learning, statistics, and data science. While NLP is not primarily focused on linguistics, many of its approaches and tasks draw on linguistic theories to address the complexities of natural language. NLP cover a wide range of tasks, including part-of-speech tagging, named entity recognition, machine translation, speech recognition, and text summarization.</p> <p>In this session, we will introduce some fundamental concepts and techniques in NLP, focusing specifically on text as the medium of natural language, rather than speech. While historically NLP has concentrated on English and a few major languages, recent years have seen a shift towards a more multilingual focus, with growing interest in low-resource languages.</p> <p>With this in mind, the session will be organized into four main blocks. We will begin by discussing the steps involved in preparing text or a corpus for NLP applications. Next, we will explore how text can be processed at three different levels: the word level (morphology), the sentence structure level (syntax), and finally, the level of sentence meaning (semantics).</p> <p>The slides for the first session are now available.</p>"},{"location":"cl/#text-preprocessing","title":"Text preprocessing","text":""},{"location":"cl/#contents-to-prepare-before-the-session-on-11212025","title":"Contents to prepare before the session on 11/21/2025","text":"<p>As mentioned earlier, this session focuses on processing textual data. Texts come from diverse sources, languages, formats, scripts, and character encoding standards. A common preliminary step in preparing text for any NLP-related task is preprocessing it to make it suitable for the specific application. Typical preprocessing tasks include removing formatting, converting character encodings, and tokenizing. Additional steps often involve normalizing text, standardizing punctuation, and similar operations.</p> <p>A helpful introduction to these strategies and their implications for various NLP tasks can be found in the article Comparison of text preprocessing methods, which you are required to read before class. You only need to read until Section 3.5, as the strategies described in Sections 3.6 and 3.7 are not that frequent for many NLP tasks, and Section 4 describes datasets that are not relevant for this session. 1.5 hours \ud83d\udd52\ufe0f in duration.</p> <p>The article focuses on tokenization at the word level. However, most neural-network-based approaches rely on subword-level tokenization, which involves splitting words into fragments ranging from single characters to character groups. Some popular subword-level tokenization techniques include byte-pair encoding (BPE), unigram, and SentencePiece.</p> <p>A concise and intuitive explanation of these methods can be found in the Tokenizers section of the HuggingFace Transformers tutorial. Scroll to the end of the page for an overview of how these subword tokenization strategies work. 0.5 hours \ud83d\udd52\ufe0f in duration.</p> <p>Additionally, if you are curious about how popular large language models handle subword tokenization, explore the Tiktokenizer. This tool simulates the tokenization process of several well-known generative neural models. Select a model from the dropdown menu in the upper-right corner and input a short text to see an example of subword tokenization. Try experimenting with texts in different languages to observe how these models manage multilingual input.</p>"},{"location":"cl/#morphological-parsing","title":"Morphological parsing","text":""},{"location":"cl/#contents-to-prepare-before-the-session-on-11212025_1","title":"Contents to prepare before the session on 11/21/2025","text":"<p>In this section, we will explore computational approaches to modeling morphology, the study of word structure. Morphological parsing involves analyzing the components of a word to understand their role in a sentence and their contribution to the meaning of a text fragment. Morphological parsing is essential for various NLP tasks, such as word segmentation and lemmatization.</p> <p>It is important to note that languages vary significantly in morphological complexity. For languages with complex morphology, morphology-aware NLP models have shown to be particularly effective, especially when only limited data is available for these languages. The paper Morphological Processing of Low-Resource Languages: Where We Are and What\u2019s Next offers a comprehensive overview of the state-of-the-art techniques in morphology analysis, with a focus on low-resource languages. These languages are especially challenging due to the scarcity of data for training state-of-the-art models. 1 hour \ud83d\udd52\ufe0f in duration.</p>"},{"location":"cl/#additional-optional-material","title":"Additional optional material:","text":"<ul> <li>An ambitious effort in recent years to create a comprehensive knowledge base for morphology across many languages is the UniMorph project. The paper UniMorph 2.0: Universal Morphology provides a detailed overview of this initiative.</li> <li>For a broader perspective on how computational approaches have addressed morphology over the decades, I recommend the chapter Computational morphology in the book What is Morphology?, edited by Mark Aronoff and Kirsten Fudeman (Wiley Blackwell, 2022). This chapter covers the evolution of computational morphology, from rule-based methods using finite-state automata to statistical approaches based on hidden Markov models.</li> </ul>"},{"location":"cl/#syntactic-parsing","title":"Syntactic parsing","text":""},{"location":"cl/#contents-to-prepare-before-the-session-on-11212025_2","title":"Contents to prepare before the session on 11/21/2025","text":"<p>Syntactic parsing involves automatically inferring the structural relationships between words in a sentence. This task is crucial for understanding the meaning of a text fragment. In this block, we will explore the technologies that enable the analysis of word relationships within sentences and how these relationships impact meaning.</p> <p>For a solid introduction to computational syntax and syntactic parsing, I recommend watching Depenency Parsing, a lecture by Graham Neubig from the Multilingual Natural Language Processing course at Carnegie Mellon University's Language Technology Institute (2022). The video is divided into two parts, but you only need to watch the first part, which ends at approximately minute 38. This lecture compares the two main syntactic parsing approaches\u2014constituent parsing and dependency parsing\u2014highlighting their advantages in multilingual contexts. It also covers key resources and tools, including the Universal Dependencies (UD) treebank, a foundational resource for multilingual dependency parsing, and discusses the primary applications of syntax in NLP. 0.75 hours  \ud83d\udd52\ufe0f in duration.</p> <p>As mentioned earlier, Universal Dependencies is one of the most widely used resources for training models in dependency parsing. Several tools and libraries leverage this resource, including the Stanza library, developed by the Stanford NLP research group. To prepare for this session, familiarize yourself with Stanza using the Multilingual Text Processing tutorial on the Applied Language Technology platform of the University of Helsinki. 1.25 hours \ud83d\udd52\ufe0f in duration.</p>"},{"location":"cl/#additional-optional-material_1","title":"Additional optional material:","text":"<p>For those interested in learning more about the Universal Dependencies project, I recommend a recent lecture by Joakim Nivre, one of the project\u2019s founders, delivered at the Institute of Formal and Applied Linguistics, Charles University (Czech Republic) in April 2024. The lecture, available as a one-hour video, provides deeper insights into the project.</p>"},{"location":"cl/#semantic-representation-of-text","title":"Semantic representation of text","text":""},{"location":"cl/#contents-to-prepare-before-the-session-on-11212025_3","title":"Contents to prepare before the session on 11/21/2025","text":"<p>This final block of the session focuses on the semantic representation of text. In NLP, there are two main approaches to representing meaning: identifying the semantic roles of text components and producing vector-based semantic representations.</p> <p>The first approach, inspired by linguistic theories of semantic analysis, involves automatically labeling roles such as experiencer, agent, theme, or goal within a text fragment. For instance, in the sentence John broke the window with a rock, we could label John as the agent, broke the window as the theme, and with a rock as the instrument. This task, known as semantic role labeling, has been crucial in NLP for many years. For example, it has enhanced machine translation systems, particularly for languages with strong divergences in the way meaning is expressed from a morphological and syntactic point of view.</p> <p>The second approach, vector semantics, focuses on building numerical representations (vectors) that capture the meaning of a text fragment. These semantic vectors are essential for various NLP tasks, including information retrieval and question answering, and they form a foundational component of neural-network-based NLP models.</p> <p>In this block, we will focus on vector semantics. For a solid introduction to the basics of this topic, refer to Chapter 6 of Speech and Language Processing by Daniel Jurafsky and James H. Martin (2024). You only need to read up to Section 6.5; some of the remaining sections will be covered in later sessions. 1 hour \ud83d\udd52\ufe0f in duration.</p>"},{"location":"speech/","title":"Architectures for Speech","text":"<p>In this module, we briefly study some neural models used to process speech. The professor for this module is Juan Antonio P\u00e9rez Ortiz.</p> <p>Class materials complement the reading of some chapters from a textbook (\"Speech and Language Processing\" by Dan Jurafsky and James H. Martin, third edition draft, available online) with annotations made by the professor.</p>"},{"location":"speech/#first-session-of-this-module-january-14-2026","title":"First session of this module (January 14, 2026)","text":""},{"location":"speech/#before-speech1","title":"Contents to prepare before the session on Jan 14","text":"<p>The activities to complete before this class are:</p> <ul> <li>Reading and studying the contents of this page on speech recognition. As you will see, the page indicates which contents you should read from the book. After a first reading, read the professor's annotations to help you understand the key concepts of the chapter. Then, perform a second reading of the chapter from the book. After finishing this part, read the description of modern architectures specific to speech recognition. In total, this part should take you about 4 hours \ud83d\udd52\ufe0f of work.</li> <li>Then, take this assessment test on these contents. There are few questions (fewer than in previous tests, in fact), and it will only take a few minutes.</li> </ul>"},{"location":"speech/#contents-for-the-in-person-session-on-jan-14","title":"Contents for the in-person session on Jan 14","text":"<p>In the in-person class (2.5 hours \ud83d\udd52\ufe0f in duration), we will see how to implement a speech classification system in PyTorch. To do this, we will use the <code>torchaudio</code> library, which is part of PyTorch. Specifically, we will briefly look at this guide to audio manipulation with torchaudio (focusing only on waveform representation and spectrogram extraction) and the implementation of the speech classifier. Both documents include links at the beginning to corresponding Google Colab notebooks. </p> <p>These two tutorials will only be covered in class to complement the theoretical contents, but you do not need to study them for the exam.</p>"},{"location":"text/","title":"Architectures for written-text processing","text":"<p>Danger</p> <p>These materials are temporary and incomplete. If you choose to read them, you do so at your own risk.</p> <p>In this module, we will study some neural models used to process texts. The professor of this module is Juan Antonio P\u00e9rez Ortiz. The module begins with a review of the functioning of logistic regression, which will help us establish the necessary knowledge to understand subsequent models. Next, we study in some detail skip-grams, one of the algorithms for obtaining non-contextual word embeddings. Then, we review the functioning of feedforward neural architectures and study their application to language models. The ultimate goal is to address the study of the most important architecture in current text processing systems: the transformer. Once we have studied these architectures, we will conclude with an analysis of the functioning of pretrained models (foundational models) in general, and language models in particular.</p> <p>Class materials complement the reading of some chapters from a textbook (\"Speech and Language Processing\" by Dan Jurafsky and James H. Martin, third edition draft, available online) with annotations made by the professor.</p>"},{"location":"text/#first-session-of-this-module-december-5-2025","title":"First session of this module (December 5, 2025)","text":""},{"location":"text/#before-text1","title":"Contents to prepare before the session on Dec 5","text":"<p>The activities to complete before this class are:</p> <ul> <li>Reading and studying the contents of this page on logistic regression. As you will see, the page indicates which contents you should read from the book. After a first reading, read the professor's annotations, whose purpose is to help you understand the key concepts of the chapter. Then, perform a second reading of the book's chapter. In total, this part should take you about 4 hours \ud83d\udd52\ufe0f of work.</li> <li>Watching and studying the video tutorials in this official PyTorch playlist. Study at least the first 4 videos (\u201cIntroduction to PyTorch\u201d, \u201cIntroduction to PyTorch Tensors\u201d, \u201cThe Fundamentals of Autograd\u201d, and \u201cBuilding Models with PyTorch\u201d). In total, this part should take you about 2 hours \ud83d\udd52\ufe0f of work.</li> <li>Reading and studying the contents of this page on embeddings. As you will see, the page indicates which contents you should read from the book. After a first reading, read the professor's annotations to help you understand the key concepts of the chapter. Then, perform a second reading of the chapter from the book. In total, this part should take you about 3 hours \ud83d\udd52\ufe0f of work.</li> <li>After completing the previous parts, take this assessment test on these contents. There are few questions, and it will take you a few minutes.</li> </ul>"},{"location":"text/#contents-for-the-in-person-session-on-dec-5","title":"Contents for the in-person session on Dec 5","text":"<p>In the in-person class (5 hours \ud83d\udd52\ufe0f long), we will see how to implement a logistic regressor in PyTorch by following the implementations of a binary logistic regressor  and a multinomial one  discussed in this section. We will also explore an implementation of the skip-gram algorithm   discussed here.</p> <p>The idea is for you to study and slightly modify the notebooks we are working on. In a later class, a more advanced assignment involving modifying the transformer's code will be presented.</p>"},{"location":"text/#second-session-december-12-2025","title":"Second session (December 12, 2025)","text":""},{"location":"text/#before-text2","title":"Contents to prepare before the session on Dec 12","text":"<p>The activities to complete before this class are:</p> <ul> <li>Reading and studying the contents of this page on feedforward neural networks and their use as very basic language models. Perform at least two readings complemented with the professor's notes as in the previous point. In total, this part should take you about 2 hours \ud83d\udd52\ufe0f of work.</li> <li>Reading and studying the contents of this page as an introduction to transformers. As always, perform at least two readings complemented with the professor's notes. In total, this part should take you about 4 hours \ud83d\udd52\ufe0f of work.</li> <li>After completing the previous parts, take this assessment test on these contents. There are few questions, and it will take you a few minutes.</li> <li>If you have time left, take the opportunity to review the contents of the first session.</li> </ul>"},{"location":"text/#contents-for-the-in-person-session-on-dec-12","title":"Contents for the in-person session on Dec 12","text":"<p>In the in-person class (5 hours \ud83d\udd52\ufe0f in duration), we will see how to implement in PyTorch a language model based on a feedforward neural network  , and a transformer   following the implementations discussed in this section and the next two.</p> <p>A very similar notebook to the one of the feedforward neural network but focused on the self-supervised learning of word embeddings   is also available.</p> <p>The idea is for you to study and slightly modify the notebooks we are working on. We will also present the assignment on mechanistic interpretability you need to submit for this module of the course.</p>"},{"location":"text/#third-session-december-19-2025","title":"Third session (December 19, 2025)","text":""},{"location":"text/#before-text3","title":"Contents to prepare before the session on Jan 19","text":"<p>The activities to complete before this class are:</p> <ul> <li>Reading and studying the contents of this page on the complete transformer model (with encoder and decoder) and the possible uses of an architecture that only includes the encoder. As you will see, the page indicates which contents you should read from the book. In particular, you will need to read some sections of the chapter on machine translation and others from the chapter on pretrained models, in addition to standalone sections on beam search and subword tokenization. After a first reading, read the professor's annotations to help you understand the key concepts of each section. Then, perform a second reading of the book's contents. In total, this part should take you about 4 hours \ud83d\udd52\ufe0f of work.</li> <li>Watching and studying Jesse Mu's lecture titled \u201cPrompting, Reinforcement Learning from Human Feedback\u201d from Stanford's CS224N course in 2023 about language models based on the transformer's decoder. This should take you about 2 hours \ud83d\udd52\ufe0f of work, as you'll need to take notes so you don't have to rewatch the video when reviewing. Downloading the slides and annotating them may be helpful. Regarding the topic discussed between minutes 39 and 46, you can simply focus on the basic ideas, as the reinforcement learning equations are not a priority topic for this course and will be covered in other courses. It's important to review what you've already studied about transformers as a language model based on the decoder before watching the video. Don't be confused by encoder-based models also sometimes being called language models. This video discusses the properties of decoder-based models initially trained to predict the next token in a sequence.</li> <li>Study the description of multilingual models in this section of one of the pages on transformers. It's a brief section that will take you about \ud83d\udd52\ufe0f 15 minutes.</li> <li>After completing the previous parts, take this assessment test on these contents. There are few questions, and it will take you a few minutes.</li> <li>If you have time left, take the opportunity to review all the contents from previous sessions.</li> </ul>"},{"location":"text/#contents-for-the-in-person-session-on-dec-19","title":"Contents for the in-person session on Dec 19","text":"<p>In the in-person class (5 hours \ud83d\udd52\ufe0f in duration), we will see how to implement on top of our transformer architecture code both a language model based on a decoder  and a named entity recognition model  based on an encoder.</p> <p>We will take the opportunity to review some aspects of the code from previous sessions and relate theoretical aspects with practical ones.</p>"},{"location":"text/#fourth-session-january-14-2026","title":"Fourth session (January 14, 2026)","text":"<p>This fourth session is actually the first and only session on the topic of speech. See the page on speech to view the contents prior to this session.</p>"}]}